2. Разработка алгоритма автоматической кластеризации текстовых документов на основе методов частотно-контекстной классификации и метода k ближайших соседей
В рамках поставленной задачи кластеризации необходимо разделить некоторое заданное множество текстовых документов D ={d_1,d_2,…,d_(|D|)} на заранее неизвестное множество возможных категорий (кластеров) С={c_1,c_2,…,c_|c| }.
Общая последовательность действий будет выглядеть следующим образом:
	Индексация документа при помощи частотного метода и определение весовых коэффициентов для каждого уникального слова в тексте.
	На этапе работы с каждым отдельным текстом необходимо определить контекст каждого ключевого слова для повышения эффективности работы алгоритма в дальнейшем.
	Определение функции «расстояния» между текстами с учетом контекста ключевых слов.
	Построение матрицы тематической близости всех текстов и применение адаптированного метода k ближайших соседей.
2.1 Применение частотного метода для первичной индексации текстов
Первоначально необходимо провести индексацию слов в каждом тексте отдельно. Для этого рассмотрим все слова текста как поток данных d_i∈D. Для удобства далее в этой части будем обозначать текст как d.
d={w_1,w_2,…,w_n},
где w_1 – информационный элемент, соответствующий конкретному слову из текста, а n = |d| – количество слов в тексте.
Множество всех уникальных для d слов обозначим как W:
W={w_1,w_2,…,w_m },
где m – количество уникальных слов в тексте d.
Вес каждого информационного элемента следует подсчитать таким образом что бы он был наиболее характерен для конкретного текста и менее характерен для других текстов и тем более текстов, относящихся к другим тематическим кластерам. Для этого используется формула TF-IDF.
В начале следует подсчитать количество повторений каждого w_1∈W в потоке данных d и рассчитывается частота данного термина в тексте по формуле:
TF= n_w/(|d| ),
где n_w – это количество употреблений конкретного слова w в документе и n_d – это количество слов во всем документе
Таким образом мы вычислим частоту каждого уникального слова в тексте. Затем вычисляется обратная частота документа IDF. Она используется для уменьшения веса общеупотребительных слов, которые не имеют непосредственного отношения к тематике конкретного текста.
IDF= 〖log〗_10 (|D|/D_w ),
где |D| – это количество текстов, а D_w – это количество тех текстов, где встречается слово w.
Итоговый весовой коэффициент каждого w будет рассчитываться по формуле:
m_i=TF*IDF
Далее необходимо провести сортировку массива W = {w_1,w_2,…,w_n} в порядке убывания весового коэффициента каждого слова w_i.

Таким образом, тематику конкретного текста в рамках задачи будет определять набор ключевых слов и их весовых коэффициентов 
T={m_1 w_1,m_2 w_2,…,m_i w_i },i=n=|W|.
Каждый элемент в множестве T является информационным признаком конкретного текста. Для того что бы сократить пространство признаков в расчет принимаются только те элементы, весовой коэффициент которых выше определенного порогового значения. Проблема заключается в определении порога (автоматизированном, машинном определении), который отделяет ключевые слова от всех остальных.
Очевидно, выбор пороговой величины должен зависеть от конкретного текста, от таких характеристик модели как m(G(W,E))Max, m(G(W,E))Min и n(W).

 
2.2 Выделение контекста для множества ключевых элементов Т
Так как смысл любого слова определяется исключительно в контексте тех слов, которые употреблялись вместе с ним, близко, рядом по тексту. И сами по себе ключевые слова в отрыве от их контекста не отражают в полной мере тематическую направленность текста. Необходимо выделить для каждого слова множество слов составляющих его окружение в рамках информационного потока d. 
Для этого определим понятие окрестности в рамках этой работы. Пускай R∈Z слов до и после ключевого слова w_i∈T считаются его контекстом. Само число R определяется в рамках адаптации алгоритма к конкретным условиям использования и определяется эмпирически. Таким образом для каждого w_i создается отдельное множество его окрестностей S не включая сам элемент w_i.
T_i  = {w_(i-R),…,w_(i-(R-1) ),w_(i+1),…,w_(i+R) },
При помощи контекста каждого ключевого слова можно существенно повысить качество кластеризации текстов, о чем будет сказано ниже.


 
2.3 Адаптация алгоритма k ближайших соседей для работы с множеством ключевых элементов и их контекстом
Для работы алгоритма k ближайших соседей необходимо разработать функцию определения расстояния между двумя текстами попарно для этого возьмем текст D как текст для которого определяется тематическая близость к тексту D_z. Для начала определим составные части этой функции.
Во первых определение тематической близости ключевых слов из двух документов:
ρ_i= m_(i min)/m_(i max) * m_i,
m_(i min)= m_i,m_(i max)= m_(zi )⇔m_i< m_zi,
m_(i min)= m_zi,m_(i max)= m_(i )⇔m_i> m_zi.
где m_i – это вес информационного элемента w_i∈D, а m_zi – это вес аналогичного ему информационного элемента w_(zi )∈ D_z.
Тематическая близость всего документа D документу D_z по ключевым словам будет вычисляться как сумма всех ρ_i
ρ= ∑_(i=1)^n▒ρ_i .
При этом ρ_i будет принимать значения от 0 до 1.

Следующей является формула определения степени сходства заранее определенных контекстов для каждого из ключевых слов. Обозначим контекст слова w_iв рамках документа D как множество Ti, а контекст этого же слова но в документе D_z как T_zi. В таком случае формула определения контекстной близости слов станет выглядеть таким образом: 
η_i= (|T∩T_z |*2)/(|T|+|T_z |)
Вычисление тематической близости с учетом контекста перепишем в следующем виде:
ρ= ∑_(i=1)^n▒〖ρ_i*η_i  〗 
2.4 Построение матрицы тематической близости текстов и применение метода k ближайших соседей
Следущим шагом будет обработка при помощи вышеописанных алгоритмов всего массива текстов каждый к каждому и получение матрицы размера i x j, где i=j=N – это количество кластеризуемых документов. 
В каждой ячейке матрицы будет значение ρ_ij, которое отображает тематическую близость i-того документа j-тому.
	d_1	d_2	***	d_(j-1)	d_j
d_1	1	ρ_1,2	***	ρ_(1,j-1)	ρ_(1,j)
d_2	ρ_2,1	1	***	ρ_(2,j-1)	ρ_(2,j)
***	***	***	***	***	***
d_(i-1)	ρ_(i-1,1)	ρ_(i-1,2)	***	1	ρ_(i-1,j)
d_i	ρ_(i,1)	ρ_(i,2)	***	ρ_(i,j-1)	1

На основании этих данных выполняется кластеризация текстов, чья схожесть выше порогового значения Q. Таким образом в результате выполнения данных операций мы получаем массив тематически организованных кластеров C.


